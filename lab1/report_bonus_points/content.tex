\section{Exercise 2.1}
Starting from the results of Experiment 4 of Assignment 1 the following approaches to improve the network performance were tested:\\
\begin{enumerate}[label=(\roman*)]
    \item use all available training data
    \item train for a longer time
\end{enumerate}
The results after Experiment 4 of Assignment 1
(lambda=0, n\_epochs=40, n\_batch=100, eta=0.1) were as follows:\\
training loss: 1.899\\
validation loss: 1.958\\
accuracy: 37.38\%\\

% \begin{table}[ht]
% \begin{tabular}{|l|l|l|l|l|}
% \hline
%                     & \textbf{Experiment 1} & \textbf{Experiment 2} & \textbf{Experiment 3} & \textbf{Experiment 4} \\ \hline
% \textbf{Training}   & 5.074                 & 1.609                 & 0.3908                & 1.899                 \\ \hline
% \textbf{Validation} & 7.526                 & 1.791                 & 1.903                 & 1.958                 \\ \hline
% \end{tabular}
% \caption{Summary of final loss}
% \label{tab:summary_loss}
% \end{table}

% \begin{table}[ht]
% \begin{tabular}{|l|l|l|l|l|}
% \hline
%                   & \textbf{Experiment 1} & \textbf{Experiment 2} & \textbf{Experiment 3} & \textbf{Experiment 4} \\ \hline
% \textbf{Accuracy} & 27.84\%               & 39.08\%               & 39.46\%               & 37.38\%               \\ \hline
% \end{tabular}
% \caption{Summary of final accuracy}
% \label{tab:summary_accuracy}
% \end{table}

\newpage

\subsection{Results Improvement (i)} 

final training loss 1.920\\
final validation loss 1.935\\
final accuracy 0.3786\\
    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001 all data for training/loss.png}
        \caption{Improvement (i) Loss (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:lossa}
    \end{figure}
    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001 all data for training/accuracy.png}
        \caption{Improvement (i) Accuracy (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:accuracya}
    \end{figure}
    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001 all data for training/weights.png}
        \caption{Improvement (i) Weights (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:weightsa}
    \end{figure}

\clearpage

\subsection{Results Improvement (ii)}
In this experiment I trained on all available data as in experiment 1 and used the same parameters. I trained the network for 2000 epochs and 
every 100 epochs I stored a snapshot of the diagrams for loss, accuracy and weights. 
Also I logged the values of each epoch to a file (result\_pics/train\_longer/values.csv). After each epoch I compared the training loss to 
the validation loss and set a threshold (0.5) to detect when the network begins to overfit, but in my runs the values stayed very close to each other 
and never exceeded that threshold. The maximum absolute difference over all 2000 epochs was 0.01996.\\
As a result we can observe that training longer does not improve the result that much, but we also see that no overfitting seems to occur. 
We see that during the first 100 epochs the accuracy increases slightly but after that it stays more or less the same.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{epoch} & \textbf{training loss} & \textbf{validation loss} & \textbf{accuracy} \\ \hline
1              & 2.35312                & 2.36131                  & 36.06\%           \\ \hline
10             & 1.92387                & 1.9376                   & 37.78\%           \\ \hline
50             & 1.92102                & 1.93453                  & 37.55\%           \\ \hline
100            & 1.92044                & 1.93162                  & 37.65\%           \\ \hline
200            & 1.91963                & 1.92828                  & 37.72\%           \\ \hline
300            & 1.92069                & 1.93098                  & 37.64\%           \\ \hline
400            & 1.92159                & 1.93381                  & 37.42\%           \\ \hline
500            & 1.92045                & 1.92874                  & 37.89\%           \\ \hline
600            & 1.9197                 & 1.93142                  & 37.90\%           \\ \hline
700            & 1.91865                & 1.93335                  & 38.25\%           \\ \hline
800            & 1.91992                & 1.93714                  & 38.37\%           \\ \hline
900            & 1.92071                & 1.93663                  & 38.03\%           \\ \hline
1000           & 1.92166                & 1.92969                  & 37.43\%           \\ \hline
\end{tabular}
\caption{Summary of longer training}
\label{tab:summary_longer_training}
\end{table}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/train_longer/2000_accuracy.png}
        \caption{Improvement (ii) accuracy after 2000 epochs}
        \label{fig:accuracyb}
    \end{figure}
    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/train_longer/2000_loss.png}
        \caption{Improvement (ii) Loss after 2000 epochs}
        \label{fig:lossb}
    \end{figure}
    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/train_longer/2000_weights.png}
        \caption{Improvement (ii) weights after 2000 epochs}
        \label{fig:weightsb}
    \end{figure}

\clearpage

\subsection{Experiment 1 diagrams}
% lambda = 0\\
% n\_epochs = 40\\
% n\_batch = 100\\
% eta = 0.1\\
As seen in the diagrams with the parameters of this run the network is not really able to learn. The loss and accuracy look very random. 
This results in a very low accuracy (27.84\%). The weight matrices look pretty random as well.

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.1/loss.png}
        \caption{Experiment 1 Loss (lambda=0, n\_epochs=40, n\_batch=100, eta=0.1)}
        \label{fig:loss1}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.1/accuracy.png}
        \caption{Experiment 1 Accuracy (lambda=0, n\_epochs=40, n\_batch=100, eta=0.1)}
        \label{fig:accuracy1}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.1/weights.png}
        \caption{Experiment 1 Weights (lambda=0, n\_epochs=40, n\_batch=100, eta=0.1)}
        \label{fig:weights1}
    \end{figure}

\clearpage
\subsection{Experiment 2 diagrams}
% lambda = 0\\
% n\_epochs = 40\\
% n\_batch = 100\\
% eta = 0.1\\
Compared to the first experiment in this run we use a much smaller learning rate. In this run it looks like the network learns better and it results in a higher
accuracy (39.08\%). If we look at the loss graphs we can see that there is a big difference between the training loss and validation loss. 
This could be an indicator for overfitting. The weight matrices look less random.

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.001/loss.png}
        \caption{Experiment 2 Loss (lambda=0, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:loss2}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.001/accuracy.png}
        \caption{Experiment 2 Accuracy (lambda=0, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:accuracy2}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=0, n_epochs=40, n_batch=100, eta=.001/weights.png}
        \caption{Experiment 2 Weights (lambda=0, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:weights2}
    \end{figure}

\clearpage
\subsection{Experiment 3 diagrams}
% lambda = 0.1\\
% n\_epochs = 40\\
% n\_batch = 100\\
% eta = 0.001\\
In this experiment we use a small regularization term. This results in a smaller training loss compared to the previous run.
But we have still a big difference between training and validation which could mean that we overfit. We start to see some more distinct patterns
in the weight matrices.

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=.1, n_epochs=40, n_batch=100, eta=.001/loss.png}
        \caption{Experiment 3 Loss (lambda=0.1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:loss3}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=.1, n_epochs=40, n_batch=100, eta=.001/accuracy.png}
        \caption{Experiment 3 Accuracy (lambda=0.1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:accuracy3}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=.1, n_epochs=40, n_batch=100, eta=.001/weights.png}
        \caption{Experiment 3 Weights (lambda=0.1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:weights3}
    \end{figure}

\clearpage
\subsection{Experiment 4 diagrams}
% lambda = 1\\
% n\_epochs = 40\\
% n\_batch = 100\\
% eta = 0.001\\
In this experiment we use an even bigger regularization term. That results in a slightly reduced accuracy but also the training 
and validation loss are close. That could mean that we are not overfitting anymore.\\
In the weight matrix we see that we start to learn some patterns that looks similar to the images of the corresponding classes. 
Maybe with more training these patterns become even more visible.

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001/loss.png}
        \caption{Experiment 4 Loss (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:loss4}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001/accuracy.png}
        \caption{Experiment 4 Accuracy (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:accuracy4}
    \end{figure}

    \begin{figure}[ht]
        \includegraphics[width=\textwidth]{../code/result_pics/lambda=1, n_epochs=40, n_batch=100, eta=.001/weights.png}
        \caption{Experiment 4 Weights (lambda=1, n\_epochs=40, n\_batch=100, eta=0.001)}
        \label{fig:weights4}
    \end{figure}

\clearpage

\section{Conclusion}
Increasing the amount of regularization reduces the accuracy but also prevents the network from overfitting. The choice of the correct
learning rate is very important for the networks ability to learn, as we can see when comparing Experiment 1 and Experiment 2. The learning rate
has a huge impact on the accuracy.